<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: infrastructure | Adventures of a wannabe geek!]]></title>
  <link href="http://www.paulstack.co.uk/blog/categories/infrastructure/atom.xml" rel="self"/>
  <link href="http://www.paulstack.co.uk/"/>
  <updated>2016-01-19T10:48:42+00:00</updated>
  <id>http://www.paulstack.co.uk/</id>
  <author>
    <name><![CDATA[Paul Stack (@stack72)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building an Autodiscovering Apache Zookeeper Cluster in AWS using Packer, Ansible and Terraform]]></title>
    <link href="http://www.paulstack.co.uk/blog/2016/01/15/building-an-autodiscovering-apache-zookeeper-cluster-in-aws-using-packer-ansible-and-terraform/"/>
    <updated>2016-01-15T15:55:00+00:00</updated>
    <id>http://www.paulstack.co.uk/blog/2016/01/15/building-an-autodiscovering-apache-zookeeper-cluster-in-aws-using-packer-ansible-and-terraform</id>
    <content type="html"><![CDATA[<p>Following my <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">pattern</a> of building <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">AMIs</a> for applications, I create my <a href="https://zookeeper.apache.org/">Apache Zookeeper</a> cluster with <a href="https://packer.io/">Packer</a> for my AMI and <a href="https://terraform.io/">Terraform</a> for the infrastructure. This Zookeeper cluster is auto-discovering of the other nodes that are determined to be in the cluster</p>

<h3>Building Zookeeper AMIs with Packer</h3>

<p>The packer template looks as follows:</p>

<p><br>
<code>json
{
  &quot;variables&quot;: {
    &quot;ami_id&quot;: &quot;&quot;,
    &quot;private_subnet_id&quot;: &quot;&quot;,
    &quot;security_group_id&quot;: &quot;&quot;,
    &quot;packer_build_number&quot;: &quot;&quot;,
  },
  &quot;description&quot;: &quot;Zookeeper Image&quot;,
  &quot;builders&quot;: [
    {
      &quot;ami_name&quot;: &quot;zookeeper-{{user `packer_build_number`}}&quot;,
      &quot;availability_zone&quot;: &quot;eu-west-1a&quot;,
      &quot;iam_instance_profile&quot;: &quot;app-server&quot;,
      &quot;instance_type&quot;: &quot;t2.small&quot;,
      &quot;region&quot;: &quot;eu-west-1&quot;,
      &quot;run_tags&quot;: {
        &quot;role&quot;: &quot;packer&quot;
      },
      &quot;security_group_ids&quot;: [
        &quot;{{user `security_group_id`}}&quot;
      ],
      &quot;source_ami&quot;: &quot;{{user `ami_id`}}&quot;,
      &quot;ssh_timeout&quot;: &quot;10m&quot;,
      &quot;ssh_username&quot;: &quot;ubuntu&quot;,
      &quot;subnet_id&quot;: &quot;{{user `private_subnet_id`}}&quot;,
      &quot;tags&quot;: {
        &quot;Name&quot;: &quot;zookeeper-packer-image&quot;
      },
      &quot;type&quot;: &quot;amazon-ebs&quot;
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;inline&quot;: [ &quot;sleep 10&quot; ]
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;install_dependencies.sh&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;&#39; | {{ .Vars }} sudo -E -S sh &#39;{{ .Path }}&#39;&quot;
    },
    {
      &quot;type&quot;: &quot;ansible-local&quot;,
      &quot;playbook_file&quot;: &quot;zookeeper.yml&quot;,
      &quot;extra_arguments&quot;: [
        &quot;--module-path=./modules&quot;
      ],
      &quot;playbook_dir&quot;: &quot;../../&quot;
    }
  ]
}
</code><br>
</p>

<p>The install_dependencies.sh script is as described <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">previously</a></p>

<p>The ansible playbook for Zookeeper looks as follows:</p>

<p>
```yaml
- hosts: all
  sudo: yes</p>

<p>pre<em>tasks:
    - ec2</em>tags:
    - ec2_facts:</p>

<p>roles:
    - base
    - zookeeper
    - exhibitor
```
</p>

<p>The base playbook installs a base role for all the base pieces of my system (e.g. Logstash, Sensu-client, prometheus node_exporter) and then proceeds to install zookeeper. As a last step, I install <a href="https://github.com/Netflix/exhibitor">exhibitor</a>. Exhibitor is a co-process for monitoring, backup/recovery, cleanup and visualization of zookeeper.</p>

<p>The zookeeper ansible role looks as follows:</p>

<p><br>
```yaml<br>
- name: Download ZooKeeper
  get<em>url: url=http://www.mirrorservice.org/sites/ftp.apache.org/zookeeper/current/zookeeper-{{ zookeeper</em>version }}.tar.gz dest=/tmp/zookeeper-{{ zookeeper_version }}.tar.gz mode=0440</p>

<ul>
<li><p>name: Unpack Zookeeper
command: tar xzf /tmp/zookeeper-{{ zookeeper<em>version }}.tar.gz -C /opt/ creates=/opt/zookeeper-{{ zookeeper</em>version }}</p></li>
<li><p>name: Link to Zookeeper Directory
file: src=/opt/zookeeper-{{ zookeeper_version }}
    dest=/opt/zookeeper
    state=link
    force=yes</p></li>
<li><p>name: Create zookeeper group
group: name=zookeeper system=true state=present</p></li>
<li><p>name: Create zookeeper user
user: name=zookeeper groups=zookeeper system=true home=/opt/zookeeper</p></li>
<li><p>name: Create Zookeeper Config Dir
file: path={{zookeeper<em>conf</em>dir}} owner=zookeeper group=zookeeper recurse=yes state=directory mode=0644</p></li>
<li><p>name: Create Zookeeper Transations Dir
file: path=/opt/zookeeper/transactions owner=zookeeper group=zookeeper recurse=yes state=directory mode=0644</p></li>
<li><p>name: Create Zookeeper Log Dir
file: path={{zookeeper<em>log</em>dir}} owner=zookeeper group=zookeeper recurse=yes state=directory mode=0644</p></li>
<li><p>name: Create Zookeeper DataStore Dir
file: path={{zookeeper<em>datastore</em>dir}} owner=zookeeper group=zookeeper recurse=yes state=directory mode=0644</p></li>
<li><p>name: Setup log4j
template: dest=&quot;{{zookeeper<em>conf</em>dir}}/log4j.properties&quot; owner=root group=root mode=644 src=log4j.properties.j2
```
</p></li>
</ul>

<p>The role itself is very simple. The zookeeper cluster is managed by exhibitor so there are very few settings passed to zookeeper at this point. One thing to note though, this requires an installation of the <a href="http://docs.oracle.com/javase/7/docs/webnotes/install/">Java JDK</a> to work.</p>

<p>The exhibitor playbook looks as follows:</p>

<p><br>
```yaml<br>
- name: Install Maven
  apt: pkg=maven state=latest update_cache=yes</p>

<ul>
<li><p>name: Create Exhibitor Install Dir
file: path={{ exhibitor<em>install</em>dir }} state=directory mode=0644</p></li>
<li><p>name: Create Exhibitor Build Dir
file: path={{ exhibitor<em>install</em>dir }}/{{ exhibitor_version }} state=directory mode=0644</p></li>
<li><p>name: Create Exhibitor POM
template: src=pom.xml.j2
        dest={{ exhibitor<em>install</em>dir }}/{{ exhibitor_version }}/pom.xml</p></li>
<li><p>name: Build Exhibitor jar
command: &#39;/usr/bin/mvn clean package -f {{ exhibitor<em>install</em>dir }}/{{ exhibitor<em>version }}/pom.xml creates={{ exhibitor</em>install<em>dir }}/{{ exhibitor</em>version }}/target/exhibitor-{{ exhibitor_version }}.jar&#39;</p></li>
<li><p>name: Copy Exhibitor jar
command: &#39;cp {{ exhibitor<em>install</em>dir }}/{{ exhibitor<em>version }}/target/exhibitor-{{ exhibitor</em>version }}.jar {{exhibitor<em>install</em>dir}}/exhibitor-standalone-{{ exhibitor<em>version }}.jar creates={{exhibitor</em>install<em>dir}}/exhibitor-standalone-{{ exhibitor</em>version }}.jar&#39;</p></li>
<li><p>name: Exhibitor Properties Config
template: src=exhibitor.properties.j2
        dest={{ exhibitor<em>install</em>dir }}/exhibitor.properties</p></li>
<li><p>name: exhibitor upstart config
template: src=upstart.j2 dest=/etc/init/exhibitor.conf mode=644 owner=root</p></li>
<li><p>service: name=exhibitor state=started
```<br>
</p></li>
</ul>

<p>This role has a lot more configuration to set as it essentially manages zookeeper. The <a href="https://gist.github.com/stack72/e5ccf1bb2bc5da484712">template files</a> for configuration are all available to download.</p>

<p>The variables for the entire playbook look as follows:</p>
<div class="highlight"><pre><code class="text">zookeeper_hosts: &quot;:2181&quot;
zookeeper_version: 3.4.6
zookeeper_conf_dir: /etc/zookeeper/conf
zookeeper_log_dir: /var/log/zookeeper
zookeeper_datastore_dir: /var/lib/zookeeper
zk_s3_bucket_name: &quot;mys3bucket&quot;
monasca_log_level: WARN
exhibitor_version: 1.5.5
exhibitor_install_dir: /opt/exhibitor
</code></pre></div>
<p>The main thing to note here is that the exhibitor process starts with the following configuration:</p>

<p>
<code>bash
exec java -jar {{ exhibitor_install_dir }}/exhibitor-standalone-{{exhibitor_version}}.jar --port 8181 --defaultconfig /opt/exhibitor/exhibitor.properties --configtype s3 --s3config {{ zk_s3_bucket_name }}:{{ ansible_ec2_placement_region }} --s3backup true --hostname {{ ec2_private_ip_address }} &gt; /var/log/exhibitor.log 2&gt;&amp;1
</code> 
</p>

<p>This means that the node will check itself into a configuration file in S3 and that all other zookeepers will read the same configuration file and can form the cluster required. You can read more about Exhibitor shared configuration on their <a href="https://github.com/Netflix/exhibitor/wiki/Shared-Configuration">github wiki</a>.</p>

<p>When I launch the instances now, the zookeeper cluster will be formed</p>

<h3>Deploying Zookeeper Infrastructure with Terraform</h3>

<p>The infrastructure of the Zookeeper cluster is pretty simple:</p>

<p>
```
resource &quot;aws<em>security</em>group&quot; &quot;zookeeper&quot; {
  name = &quot;digit-zookeeper-sg&quot;
  description = &quot;Zookeeper Security Group&quot;
  vpc<em>id = &quot;${aws</em>vpc.default.id}&quot;</p>

<p>ingress {
    from<em>port = 0
    to</em>port   = 0
    protocol  = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>egress {
    from<em>port = &quot;0&quot;
    to</em>port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>tags {
    Name = &quot;ZooKeeper Node&quot;
  }
}</p>

<p>resource &quot;aws<em>launch</em>configuration&quot; &quot;zookeeper<em>launch</em>config&quot; {
  image<em>id = &quot;${var.zookeeper</em>ami<em>id}&quot;
  instance</em>type = &quot;${var.zookeeper<em>instance</em>type}&quot;
  iam<em>instance</em>profile = &quot;zookeeper-server&quot;
  key<em>name = &quot;${aws</em>key<em>pair.terraform.key</em>name}&quot;
  security<em>groups = [&quot;${aws</em>security<em>group.zookeeper.id}&quot;,&quot;${aws</em>security<em>group.node.id}&quot;]
  enable</em>monitoring = false</p>

<p>lifecycle {
    create<em>before</em>destroy = true
  }</p>

<p>root<em>block</em>device {
    volume<em>size = &quot;${var.digit</em>zookeeper<em>volume</em>size}&quot;
  }
}</p>

<p>resource &quot;aws<em>autoscaling</em>group&quot; &quot;zookeeper<em>autoscale</em>group&quot; {
  name = &quot;zookeeper-autoscale-group&quot;
  availability<em>zones = [&quot;${aws</em>subnet.primary-private.availability<em>zone}&quot;,&quot;${aws</em>subnet.secondary-private.availability<em>zone}&quot;,&quot;${aws</em>subnet.tertiary-private.availability<em>zone}&quot;]
  vpc</em>zone<em>identifier = [&quot;${aws</em>subnet.primary-private.id}&quot;,&quot;${aws<em>subnet.secondary-private.id}&quot;,&quot;${aws</em>subnet.tertiary-private.id}&quot;]  launch<em>configuration = &quot;${aws</em>launch<em>configuration.zookeeper</em>launch<em>config.id}&quot;
  min</em>size = 0
  max_size = 100
  desired = 3</p>

<p>tag {
    key = &quot;Name&quot;
    value = &quot;zookeeper&quot;
    propagate<em>at</em>launch = true
  }</p>

<p>tag {
    key = &quot;role&quot;
    value = &quot;zookeeper&quot;
    propagate<em>at</em>launch = true
  }
}</p>
<div class="highlight"><pre><code class="text">

When Terraform is applied here, a 3 node cluster of zookeeper will be created. You can go to [exhibitor](http://mynodeip:8181/exhibitor/v1/ui/index.html) and see the configuration e.g.

![Image](/images/exhibitor-zookeeper-cluster.png)
![Image](/images/exhibitor-zookeeper-cluster-config.png)
</code></pre></div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replacing a node in a Riak Cluster]]></title>
    <link href="http://www.paulstack.co.uk/blog/2016/01/15/replacing-a-node-in-a-riak-cluster/"/>
    <updated>2016-01-15T11:03:00+00:00</updated>
    <id>http://www.paulstack.co.uk/blog/2016/01/15/replacing-a-node-in-a-riak-cluster</id>
    <content type="html"><![CDATA[<p>The instances that run in my infrastructure get a lifespan of 14 days. This allows me to continually test that I can replace my environment at any point. People always ask me if I follow the same principal for data nodes. I posted <a href="http://www.paulstack.co.uk/blog/2016/01/04/replacing-the-nodes-in-an-aws-elasticsearch-cluster/">previously</a> about replacing nodes is an <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> cluster, this post will detail how I replace  nodes in a <a href="http://basho.com/products/riak-kv/">Riak</a> cluster </p>

<p><strong><em>NOTE</em></strong>: This post assumes that you have the <a href="http://docs.basho.com/riak/latest/ops/advanced/riak-control/">Riak Control console</a> enabled for Riak. You can find out how to enable that in the <a href="http://www.paulstack.co.uk/blog/2016/01/06/building-a-riak-cluster-in-aws-with-packer-and-terraform/">post</a> I wrote on configuring Riak.</p>

<p>When going to the Riak Control, you can find the following screens:</p>

<p><em>Cluster Health</em></p>

<p><img src="/images/riak-control-cluster-health.png" alt="Image"></p>

<p><em>Ring Status</em></p>

<p><img src="/images/riak-control-ring-status.png" alt="Image"></p>

<p><em>Cluster Management</em> </p>

<p><img src="/images/riak-control-cluster-mgmt.png" alt="Image"></p>

<p><em>Node Management</em></p>

<p><img src="/images/riak-control-node-mgmt.png" alt="Image"></p>

<h3>Removing a node from the Cluster</h3>

<p>In order to remove a node from the cluster, go to the cluster managemenet screen. Find the node you want to replace in the list and click on the <code>Actions</code> toggle. It will reveal actions as follows:</p>

<p><img src="/images/riak-control-cluster-node-options.png" alt="Image"></p>

<p>As the node is currently running, I tend to chose the <code>Allow this node to leave normally</code> option (if the node had died or was unresponsive, I would usually chose the <code>force this node to leave</code>). Clicking on the <code>Stage</code> button, details a plan of what is going to happen:</p>

<p><img src="/images/riak-control-remove-node.png" alt="Image"></p>

<p>If the proposed changes look good, <code>Commit</code> the plan. Watch the partitions drain from the node to be replaced:</p>

<p><img src="/images/riak-control-node-draining.png" alt="Image"></p>

<p>When the all the partitions have drained, we now have a 2 node cluster where the partitons are split 50:50:</p>

<p><img src="/images/riak-control-smaller-cluster.png" alt="Image"></p>

<p>We can now destroy the node and let the <a href="https://aws.amazon.com/autoscaling/">autoscaling group</a> launch another to replace it</p>

<h3>Adding a new node to the Cluster</h3>

<p>Assuming a new node has already been launched and is ready to go into the cluster. Go to the cluster management page in the portal and enter new node details. It should follow the format <code>riak@&lt;ipaddress&gt;</code></p>

<p><img src="/images/riak-control-add-new-node.png" alt="Image"></p>

<p>The list of actions that are pending on the cluster:</p>

<p><img src="/images/riak-control-stage-new-node.png" alt="Image"></p>

<p><code>Commit</code> the changes, watch the partions rebalance across the cluster:</p>

<p><img src="/images/riak-control-cluster-rebalance.png" alt="Image"></p>

<p>The cluster will return to being 3 nodes, with equal partition split and will then show as green again</p>

<p><img src="/images/riak-control-green-cluster.png" alt="Image"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a Riak Cluster in AWS with Packer and Terraform]]></title>
    <link href="http://www.paulstack.co.uk/blog/2016/01/06/building-a-riak-cluster-in-aws-with-packer-and-terraform/"/>
    <updated>2016-01-06T15:02:00+00:00</updated>
    <id>http://www.paulstack.co.uk/blog/2016/01/06/building-a-riak-cluster-in-aws-with-packer-and-terraform</id>
    <content type="html"><![CDATA[<p>Following my <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">pattern</a> of building <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">AMIs</a> for applications, I create my <a href="http://basho.com/products/riak-kv/">riak</a> cluster with <a href="https://packer.io/">Packer</a> for my AMI and <a href="https://terraform.io/">Terraform</a> for the infrastructure</p>

<h3>Building Riak AMIs with Packer</h3>

<p>
<code>json
{
  &quot;variables&quot;: {
    &quot;ami_id&quot;: &quot;&quot;,
    &quot;private_subnet_id&quot;: &quot;&quot;,
    &quot;security_group_id&quot;: &quot;&quot;,
    &quot;packer_build_number&quot;: &quot;&quot;,
  },
  &quot;description&quot;: &quot;Riak Image&quot;,
  &quot;builders&quot;: [
    {
      &quot;ami_name&quot;: &quot;riak-{{user `packer_build_number`}}&quot;,
      &quot;availability_zone&quot;: &quot;eu-west-1a&quot;,
      &quot;iam_instance_profile&quot;: &quot;app-server&quot;,
      &quot;instance_type&quot;: &quot;t2.small&quot;,
      &quot;region&quot;: &quot;eu-west-1&quot;,
      &quot;run_tags&quot;: {
        &quot;role&quot;: &quot;packer&quot;
      },
      &quot;security_group_ids&quot;: [
        &quot;{{user `security_group_id`}}&quot;
      ],
      &quot;source_ami&quot;: &quot;{{user `ami_id`}}&quot;,
      &quot;ssh_timeout&quot;: &quot;10m&quot;,
      &quot;ssh_username&quot;: &quot;ubuntu&quot;,
      &quot;subnet_id&quot;: &quot;{{user `private_subnet_id`}}&quot;,
      &quot;tags&quot;: {
        &quot;Name&quot;: &quot;riak-packer-image&quot;
      },
      &quot;type&quot;: &quot;amazon-ebs&quot;
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;inline&quot;: [ &quot;sleep 10&quot; ]
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;install_dependencies.sh&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;&#39; | {{ .Vars }} sudo -E -S sh &#39;{{ .Path }}&#39;&quot;
    },
    {
      &quot;type&quot;: &quot;ansible-local&quot;,
      &quot;playbook_file&quot;: &quot;riak.yml&quot;,
      &quot;extra_arguments&quot;: [
        &quot;--module-path=./modules&quot;
      ],
      &quot;playbook_dir&quot;: &quot;../../&quot;
    }
  ]
}
</code>
</p>

<p>The install_dependencies.sh script is as described <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">previously</a></p>

<p>The ansible playbook for Riak looks as follows:</p>

<p>
```yaml
- hosts: all
  sudo: yes</p>

<p>pre<em>tasks:
    - ec2</em>tags:
    - ec2_facts:</p>

<p>roles:
    - base
    - riak
```
</p>

<p>The base playbook installs a base role for all the base pieces of my system (e.g. Logstash, Sensu-client, prometheus node_exporter) and then proceeds to install riak.</p>

<p>The riak ansible role looks as follows:</p>

<p>
```yaml
- action: apt<em>key url={{ riak</em>key_url }} state=present</p>

<ul>
<li><p>action: apt<em>repository repo=&#39;{{ riak</em>deb<em>repo }}&#39; state=present update</em>cache=yes</p></li>
<li><p>apt: name=riak={{ riak<em>version }} state=present update</em>cache=yes</p></li>
<li><p>name: set ulimit
copy: src=etc-default-riak dest=/etc/default/riak owner=root group=root mode=0644</p></li>
<li><p>name: template riak configuration
template: src=riak.j2 dest=/etc/riak/riak.conf owner=riak mode=0644
register: restart_riak</p></li>
<li><p>name: restart riak
service: name=riak state=started
```
</p></li>
</ul>

<p>The role itself is very simple. The riak cluster settings are all held in the <a href="https://gist.github.com/stack72/e0df60305aff136cb81c">riak.j2</a> template file. Notice that the riak template has the following line in it:</p>
<div class="highlight"><pre><code class="text">riak_control = on
</code></pre></div>
<p>The variables for the riak playbook look as follows:</p>
<div class="highlight"><pre><code class="text">riak_key_url: &quot;https://packagecloud.io/gpg.key&quot;
riak_deb_repo: &quot;deb https://packagecloud.io/basho/riak/ubuntu/ trusty main&quot;
riak_version: 2.1.1-1
</code></pre></div>
<h3>Deploying Riak with Terraform</h3>

<p>The infrastructure of the Riak cluster is pretty simple:</p>

<p>
```
resource &quot;aws<em>elb&quot; &quot;riak</em>v2<em>elb&quot; {
  name = &quot;riak-elb-v2&quot;
  subnets = [&quot;${aws</em>subnet.primary-private.id}&quot;,&quot;${aws<em>subnet.secondary-private.id}&quot;,&quot;${aws</em>subnet.tertiary-private.id}&quot;]
  security<em>groups = [&quot;${aws</em>security<em>group.riak</em>elb.id}&quot;]
  cross<em>zone</em>load<em>balancing = true
  connection</em>draining = true
  internal = true</p>

<p>listener {
    instance<em>port      = 8098
    instance</em>protocol  = &quot;tcp&quot;
    lb<em>port            = 8098
    lb</em>protocol        = &quot;tcp&quot;
  }</p>

<p>health<em>check {
    healthy</em>threshold   = 2
    unhealthy_threshold = 2
    interval            = 10
    target              = &quot;HTTP:8098/ping&quot;
    timeout             = 5
  }
}</p>

<p>resource &quot;aws<em>security</em>group&quot; &quot;riak&quot; {
  name = &quot;riak-sg&quot;
  description = &quot;Riak Security Group&quot;
  vpc<em>id = &quot;${aws</em>vpc.default.id}&quot;</p>

<p>ingress {
    from<em>port = 8098
    to</em>port   = 8098
    protocol  = &quot;tcp&quot;
    security<em>groups = [&quot;${aws</em>security<em>group.riak</em>elb.id}&quot;]
  }</p>

<p>ingress {
    from<em>port = 8098
    to</em>port   = 8098
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>egress {
    from<em>port = &quot;0&quot;
    to</em>port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>tags {
    Name = &quot;Riak Node&quot;
  }
}</p>

<p>resource &quot;aws<em>security</em>group<em>rule&quot; &quot;riak</em>all<em>tcp&quot; {
    type = &quot;ingress&quot;
    from</em>port = 0
    to<em>port = 65535
    protocol = &quot;tcp&quot;
    security</em>group<em>id = &quot;${aws</em>security<em>group.riak.id}&quot;
    source</em>security<em>group</em>id = &quot;${aws<em>security</em>group.riak.id}&quot;
}</p>

<p>resource &quot;aws<em>security</em>group&quot; &quot;riak<em>elb&quot; {
  name = &quot;riak-elb-sg&quot;
  description = &quot;Riak Elastic Load Balancer Security Group&quot;
  vpc</em>id = &quot;${aws_vpc.default.id}&quot;</p>

<p>ingress {
    from<em>port = 8098
    to</em>port   = 8098
    protocol  = &quot;tcp&quot;
    security<em>groups = [&quot;${aws</em>security_group.node.id}&quot;]
  }</p>

<p>ingress {
    from<em>port = 8098
    to</em>port   = 8098
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>egress {
    from<em>port = &quot;0&quot;
    to</em>port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }</p>

<p>tags {
    Name = &quot;Riak Load Balancer&quot;
  }
}</p>

<p>resource &quot;aws<em>autoscaling</em>group&quot; &quot;riak<em>v2</em>autoscale<em>group&quot; {
  name = &quot;riak-v2-autoscale-group&quot;
  availability</em>zones = [&quot;${aws<em>subnet.primary-private.availability</em>zone}&quot;,&quot;${aws<em>subnet.secondary-private.availability</em>zone}&quot;,&quot;${aws<em>subnet.tertiary-private.availability</em>zone}&quot;]
  vpc<em>zone</em>identifier = [&quot;${aws<em>subnet.primary-private.id}&quot;,&quot;${aws</em>subnet.secondary-private.id}&quot;,&quot;${aws<em>subnet.tertiary-private.id}&quot;]
  launch</em>configuration = &quot;${aws<em>launch</em>configuration.riak<em>launch</em>config.id}&quot;
  min<em>size = 0
  max</em>size = 100
  health<em>check</em>type = &quot;EC2&quot;</p>

<p>tag {
    key = &quot;Name&quot;
    value = &quot;riak&quot;
    propagate<em>at</em>launch = true
  }</p>

<p>tag {
    key = &quot;role&quot;
    value = &quot;riak&quot;
    propagate<em>at</em>launch = true
  }</p>

<p>tag {
    key = &quot;elb<em>name&quot;
    value = &quot;${aws</em>elb.riak<em>v2</em>elb.name}&quot;
    propagate<em>at</em>launch = true
  }</p>

<p>tag {
    key = &quot;elb<em>region&quot;
    value = &quot;${var.aws</em>region}&quot;
    propagate<em>at</em>launch = true
  }
}</p>

<p>resource &quot;aws<em>launch</em>configuration&quot; &quot;riak<em>launch</em>config&quot; {
  image<em>id = &quot;${var.riak</em>ami<em>id}&quot;
  instance</em>type = &quot;${var.riak<em>instance</em>type}&quot;
  iam<em>instance</em>profile = &quot;app-server&quot;
  key<em>name = &quot;${aws</em>key<em>pair.terraform.key</em>name}&quot;
  security<em>groups = [&quot;${aws</em>security<em>group.riak.id}&quot;,&quot;${aws</em>security<em>group.node.id}&quot;]
  enable</em>monitoring = false</p>

<p>lifecycle {
    create<em>before</em>destroy = true
  }</p>

<p>root<em>block</em>device {
    volume<em>size = &quot;${var.driak</em>volume_size}&quot;
  }
}</p>
<div class="highlight"><pre><code class="text">
</code></pre></div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replacing the Nodes in an AWS ElasticSearch Cluster]]></title>
    <link href="http://www.paulstack.co.uk/blog/2016/01/04/replacing-the-nodes-in-an-aws-elasticsearch-cluster/"/>
    <updated>2016-01-04T17:01:00+00:00</updated>
    <id>http://www.paulstack.co.uk/blog/2016/01/04/replacing-the-nodes-in-an-aws-elasticsearch-cluster</id>
    <content type="html"><![CDATA[<p>In a <a href="http://www.paulstack.co.uk/blog/2015/11/09/the-quest-for-infra-management-2-dot-0/">previous post</a>, I talked about how I have tended towards the philosophy of &#39;Immutable Infrastructure&#39;. As part of that philospohy, when a box is created in my environment, it has a lifespan of 14 days. On the 14th day, I get a notification to tell me that the box is due for renewal. When it comes to <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> nodes, there is a process I follow to renew a box. </p>

<p>I have an example 3 nodes cluster of ElasticSearch up and running to test this on:</p>

<p><img src="/images/aws_elasticsearch_cluster.png" alt="Image"></p>

<p>Let&#39;s assume that instance <em>i-</em> was due for renewal. Firstly, I would usually disable shard reallocation. This will stop unnecessary data transfer between nodes and minimise the wastage of I/O.</p>
<div class="highlight"><pre><code class="text">curl -XPUT localhost:9200/_cluster/settings -d &#39;{
                &quot;transient&quot; : {
                    &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;
                }
        }&#39;
</code></pre></div>
<p>As these shard allocation is now disabled, I can continue with the node replacement. There are a few ways to do this. Previously to ElasticSearch 2.0, we could do it with the ElasticSearch API:</p>
<div class="highlight"><pre><code class="text">curl -XPOST &#39;http://localhost:9200/_cluster/nodes/MYNODEIP/_shutdown&#39;
</code></pre></div>
<p>If you are using ElasticSearch 2.0, you are more than likely running ElasticSearch as a service. To shutdown the node, stop the service.</p>

<p>By looking at the status of the cluster now, I can see the following:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?pretty=true&#39;
{
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;status&quot; : &quot;yellow&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 2,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 160,
  &quot;active_shards&quot; : 317,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 2,
  &quot;unassigned_shards&quot; : 151,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0
}
</code></pre></div>
<p>I can see that it tells me the cluster is yellow and that I have 2 nodes in it. I can proceed with the instance termination.</p>

<p><img src="/images/aws_elasticsearch_instance_terminated.png" alt="Image"></p>

<p>I have an AWS <a href="https://aws.amazon.com/autoscaling/">Autoscale Group</a> configured for ElasticSearch to keep 3 instances running. Therefore, the node that I destroyed will fail the Autoscale Group Healthcheck and a new instance will be spawned to replace it.</p>

<p>Using the ElasticSearch <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html">Cluster Health API</a>, I can determine when the new node is in place:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?wait_for_nodes=3&amp;timeout=100s&#39;
</code></pre></div>
<p>The command will continue running until the cluster has 3 nodes in it. If you want to replace more nodes in the cluster, then repeat the steps above. If you are finished, then it is important to re-enable the shard reallocation:</p>
<div class="highlight"><pre><code class="text">curl -XPUT localhost:9200/_cluster/settings -d &#39;{
                &quot;transient&quot; : {
                    &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot;
                }
        }&#39;
</code></pre></div>
<p>The time taken to rebalance the cluster will depend on the number and size of the shards.</p>

<p>You can monitor the health of the cluster until it turns green:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?wait_for_status=green&amp;timeout=100s&#39;
</code></pre></div>
<p>The cluster is now green and all is working as expected again:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?pretty=true&#39;
{
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 3,
  &quot;number_of_data_nodes&quot; : 3,
  &quot;active_primary_shards&quot; : 160,
  &quot;active_shards&quot; : 470,
  &quot;relocating_shards&quot; : 1,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0
}
</code></pre></div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Kibana Using Nginx as an SSL Proxy]]></title>
    <link href="http://www.paulstack.co.uk/blog/2016/01/03/deploying-kibana-using-nginx-as-an-ssl-proxy/"/>
    <updated>2016-01-03T01:16:00+00:00</updated>
    <id>http://www.paulstack.co.uk/blog/2016/01/03/deploying-kibana-using-nginx-as-an-ssl-proxy</id>
    <content type="html"><![CDATA[<p>In my last <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">post</a>, I described how I use <a href="https://packer.io/">Packer</a> and <a href="https://terraform.io/">Terraform</a> to deploy an <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> cluster. In order to make the logs stored in ElasticSearch searchable, I use <a href="https://www.elastic.co/products/kibana">Kibana</a>. I follow the previous pattern and deploy Kibana using Packer to build an <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.htm">AMI</a> and then create the infrastructure using Terraform. The Packer template has already taken into account that I want to use <a href="https://www.nginx.com/">nginx</a> as a proxy. </p>

<h3>Building Kibana AMIs with Packer and Ansible</h3>

<p>The template looks as follows:</p>

<p>
<code>json 
{
  &quot;variables&quot;: {
    &quot;ami_id&quot;: &quot;&quot;,
    &quot;private_subnet_id&quot;: &quot;&quot;,
    &quot;security_group_id&quot;: &quot;&quot;,
    &quot;packer_build_number&quot;: &quot;&quot;,
  },
  &quot;description&quot;: &quot;Kibana Image&quot;,
  &quot;builders&quot;: [
    {
      &quot;ami_name&quot;: &quot;kibana-{{user `packer_build_number`}}&quot;,
      &quot;availability_zone&quot;: &quot;eu-west-1a&quot;,
      &quot;iam_instance_profile&quot;: &quot;app-server&quot;,
      &quot;instance_type&quot;: &quot;t2.small&quot;,
      &quot;region&quot;: &quot;eu-west-1&quot;,
      &quot;run_tags&quot;: {
        &quot;role&quot;: &quot;packer&quot;
      },
      &quot;security_group_ids&quot;: [
        &quot;{{user `security_group_id`}}&quot;
      ],
      &quot;source_ami&quot;: &quot;{{user `ami_id`}}&quot;,
      &quot;ssh_timeout&quot;: &quot;10m&quot;,
      &quot;ssh_username&quot;: &quot;ubuntu&quot;,
      &quot;subnet_id&quot;: &quot;{{user `private_subnet_id`}}&quot;,
      &quot;tags&quot;: {
        &quot;Name&quot;: &quot;kibana-packer-image&quot;
      },
      &quot;type&quot;: &quot;amazon-ebs&quot;
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;inline&quot;: [ &quot;sleep 10&quot; ]
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;install_dependencies.sh&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;&#39; | {{ .Vars }} sudo -E -S sh &#39;{{ .Path }}&#39;&quot;
    },
    {
      &quot;type&quot;: &quot;ansible-local&quot;,
      &quot;playbook_file&quot;: &quot;kibana.yml&quot;,
      &quot;extra_arguments&quot;: [
        &quot;--module-path=./modules&quot;
      ],
      &quot;playbook_dir&quot;: &quot;../../&quot;
    }
  ]
}
</code>
 </p>

<p>The install_dependencies.sh script is as described <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">previously</a></p>

<p>The ansible playbook for Kibana looks as follows:  </p>

<p>
```yaml
- hosts: all
  sudo: yes</p>

<p>pre<em>tasks:
    - ec2</em>tags:
    - ec2_facts:</p>

<p>roles:
    - base
    - kibana
    - reverse_proxied
```
</p>

<p>The playbook installs a base role for all the base pieces of my system (e.g. Logstash, Sensu-client, prometheus node_exporter) and then proceeds to install ElasticSearch.</p>

<p>The Kibana role looks as follows:</p>

<p><br>
```yaml<br>
- name: Download Kibana
  get<em>url: url=https://download.elasticsearch.org/kibana/kibana/kibana-{{ kibana</em>version }}-linux-x64.tar.gz dest=/tmp/kibana-{{ kibana_version }}-linux-x64.tar.gz mode=0440</p>

<ul>
<li><p>name: Untar Kibana
command: tar xzf /tmp/kibana-{{ kibana<em>version }}-linux-x64.tar.gz -C /opt creates=/opt/kibana-{{ kibana</em>version }}-linux-x64.tar.gz</p></li>
<li><p>name: Link to Kibana Directory
file: src=/opt/kibana-{{ kibana_version }}-linux-x64
    dest=/opt/kibana
    state=link
    force=yes</p></li>
<li><p>name: Link Kibana to ElasticSearch
lineinfile: &gt;
dest=/opt/kibana/config/kibana.yml
regexp=&quot;^elasticsearch<em>url:&quot;
line=&#39;elasticsearch</em>url: &quot;{{ elasticsearch_url }}&quot;&#39;</p></li>
<li><p>name: Create Kibana Init Script
copy: src=initd.conf dest=/etc/init.d/kibana mode=755 owner=root</p></li>
<li><p>name: Ensure Kibana is running
service: name=kibana state=started
```
</p></li>
</ul>

<p>The reverse_proxied ansible role looks as follows:</p>

<p>
```yaml
- name: download private key file
  command: aws s3 cp {{ reverse<em>proxy</em>private<em>key</em>s3<em>path }} /etc/ssl/private/{{ reverse</em>proxy<em>private</em>key }}</p>

<ul>
<li><p>name: private key permissions
file: path=/etc/ssl/private/{{ reverse<em>proxy</em>private_key }} mode=600</p></li>
<li><p>name: download certificate file
command: aws s3 cp {{ reverse<em>proxy</em>cert<em>s3</em>path }} /etc/ssl/certs/{{ reverse<em>proxy</em>cert }}</p></li>
<li><p>name: download DH 2048bit encryption
command: aws s3 cp {{ reverse<em>proxy</em>dh<em>pem</em>s3<em>path }} /etc/ssl/{{ reverse</em>proxy<em>dh</em>pem }}</p></li>
<li><p>name: certificate permissions
file: path=/etc/ssl/certs/{{ reverse<em>proxy</em>cert }} mode=644</p></li>
<li><p>apt: pkg=nginx</p></li>
<li><p>name: remove default nginx site from sites-emabled
file: path=/etc/nginx/sites-enabled/default state=absent</p></li>
<li><p>template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf mode=644 owner=root group=root</p></li>
<li><p>service: name=nginx state=restarted</p></li>
<li><p>file: path=/var/log/nginx
    mode=0755
    state=directory
```
</p></li>
</ul>

<p>This role downloads a private SSL Key and a Certificate from a <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html">S3 bucket</a> that is security controlled through <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">IAM</a>. This allows us to configure nginx to act as a proxy. The nginx proxy template is available to <a href="https://gist.github.com/stack72/1d76839c6783bd7eea33">view</a>.</p>

<p>We can then pass a number of variables to our role for use within ansible:</p>

<p>
<code>yaml
reverse_proxy_private_key: mydomain.key
reverse_proxy_private_key_s3_path: s3://my-bucket/certs/mydomain/mydomain.key
reverse_proxy_cert: mydomain.crt
reverse_proxy_cert_s3_path: s3://my-bucket/certs/mydomain/mydomain.crt
reverse_proxy_dh_pem_s3_path: s3://my-bucket/certs/dhparams.pem
reverse_proxy_dh_pem: dhparams.pem
proxy_urls:
  - reverse_proxy_url: /
    reverse_proxy_upstream_port: 3000
kibana_version: 4.1.0
elasticsearch_url: http://myes.com:9200
</code>
</p>

<p>This allows me to easily change the configuration of nginx to patch security vulnerabilities easily.</p>

<h3>Deploying Kibana with Terraform</h3>

<p>The infrastructure of the Kibana cluster is now pretty easy. The Terraform script now looks as follows:</p>
<div class="highlight"><pre><code class="text">resource &quot;aws_security_group&quot; &quot;kibana&quot; {
  name = &quot;kibana-sg&quot;
  description = &quot;Kibana Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 443
    to_port   = 443
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  }

  ingress {
    from_port = 80
    to_port   = 80
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;Kibana Node&quot;
  }
}

resource &quot;aws_security_group&quot; &quot;kibana_elb&quot; {
  name = &quot;kibana-elb-sg&quot;
  description = &quot;Kibana Elastic Load Balancer Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 443
    to_port   = 443
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  ingress {
    from_port = 80
    to_port   = 80
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;Kibana Load Balancer&quot;
  }
}

resource &quot;aws_elb&quot; &quot;kibana_elb&quot; {
  name = &quot;kibana-elb&quot;
  subnets = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  cross_zone_load_balancing = true
  connection_draining = true
  internal = true

  listener {
    instance_port      = 443
    instance_protocol  = &quot;tcp&quot;
    lb_port            = 443
    lb_protocol        = &quot;tcp&quot;
  }

  listener {
    instance_port      = 80
    instance_protocol  = &quot;tcp&quot;
    lb_port            = 80
    lb_protocol        = &quot;tcp&quot;
  }

  health_check {
    healthy_threshold   = 2
    unhealthy_threshold = 2
    interval            = 10
    target              = &quot;TCP:443&quot;
    timeout             = 5
  }
}

resource &quot;aws_launch_configuration&quot; &quot;kibana_launch_config&quot; {
  image_id = &quot;${var.kibana_ami_id}&quot;
  instance_type = &quot;${var.kibana_instance_type}&quot;
  iam_instance_profile = &quot;app-server&quot;
  key_name = &quot;${aws_key_pair.terraform.key_name}&quot;
  security_groups = [&quot;${aws_security_group.kibana.id}&quot;,&quot;${aws_security_group.node.id}&quot;]
  enable_monitoring = false

  root_block_device {
    volume_size = &quot;${var.kibana_volume_size}&quot;
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource &quot;aws_autoscaling_group&quot; &quot;kibana_autoscale_group&quot; {
  name = &quot;kibana-autoscale-group&quot;
  availability_zones = [&quot;${aws_subnet.primary-private.availability_zone}&quot;,&quot;${aws_subnet.secondary-private.availability_zone}&quot;,&quot;${aws_subnet.tertiary-private.availability_zone}&quot;]
  vpc_zone_identifier = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  launch_configuration = &quot;${aws_launch_configuration.kibana_launch_config.id}&quot;
  min_size = 2
  max_size = 100
  health_check_type = &quot;EC2&quot;
  load_balancers = [&quot;${aws_elb.kibana_elb.name}&quot;]

  tag {
    key = &quot;Name&quot;
    value = &quot;kibana&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;role&quot;
    value = &quot;kibana&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_name&quot;
    value = &quot;${aws_elb.kibana_elb.name}&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_region&quot;
    value = &quot;${var.aws_region}&quot;
    propagate_at_launch = true
  }
}
</code></pre></div>
<p>This allows me to scale my system up or down just by changing the values in my Terraform configuration. When the instances are instantiated, the Kibana instances are added to the ELB and they are then available to serve traffic</p>
]]></content>
  </entry>
  
</feed>
