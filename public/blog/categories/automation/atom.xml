<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: automation | Adventures of a wannabe geek!]]></title>
  <link href="http://paulstack.co.uk/blog/categories/automation/atom.xml" rel="self"/>
  <link href="http://paulstack.co.uk/"/>
  <updated>2016-01-06T12:57:48+00:00</updated>
  <id>http://paulstack.co.uk/</id>
  <author>
    <name><![CDATA[Paul Stack (@stack72)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Replacing the Nodes in an AWS ElasticSearch Cluster]]></title>
    <link href="http://paulstack.co.uk/blog/2016/01/04/replacing-the-nodes-in-an-aws-elasticsearch-cluster/"/>
    <updated>2016-01-04T17:01:00+00:00</updated>
    <id>http://paulstack.co.uk/blog/2016/01/04/replacing-the-nodes-in-an-aws-elasticsearch-cluster</id>
    <content type="html"><![CDATA[<p>In a <a href="http://www.paulstack.co.uk/blog/2015/11/09/the-quest-for-infra-management-2-dot-0/">previous post</a>, I talked about how I have tended towards the philosophy of &#39;Immutable Infrastructure&#39;. As part of that philospohy, when a box is created in my environment, it has a lifespan of 14 days. On the 14th day, I get a notification to tell me that the box is due for renewal. When it comes to <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> nodes, there is a process I follow to renew a box. </p>

<p>I have an example 3 nodes cluster of ElasticSearch up and running to test this on:</p>

<p><img src="/images/aws_elasticsearch_cluster.png" alt="Image"></p>

<p>Let&#39;s assume that instance <em>i-</em> was due for renewal. Firstly, I would usually disable shard reallocation. This will stop unnecessary data transfer between nodes and minimise the wastage of I/O.</p>
<div class="highlight"><pre><code class="text">curl -XPUT localhost:9200/_cluster/settings -d &#39;{
                &quot;transient&quot; : {
                    &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;
                }
        }&#39;
</code></pre></div>
<p>As these shard allocation is now disabled, I can continue with the node replacement. There are a few ways to do this. Previously to ElasticSearch 2.0, we could do it with the ElasticSearch API:</p>
<div class="highlight"><pre><code class="text">curl -XPOST &#39;http://localhost:9200/_cluster/nodes/MYNODEIP/_shutdown&#39;
</code></pre></div>
<p>If you are using ElasticSearch 2.0, you are more than likely running ElasticSearch as a service. To shutdown the node, stop the service.</p>

<p>By looking at the status of the cluster now, I can see the following:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?pretty=true&#39;
{
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;status&quot; : &quot;yellow&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 2,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 160,
  &quot;active_shards&quot; : 317,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 2,
  &quot;unassigned_shards&quot; : 151,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0
}
</code></pre></div>
<p>I can see that it tells me the cluster is yellow and that I have 2 nodes in it. I can proceed with the instance termination.</p>

<p><img src="/images/aws_elasticsearch_instance_terminated.png" alt="Image"></p>

<p>I have an AWS <a href="https://aws.amazon.com/autoscaling/">Autoscale Group</a> configured for ElasticSearch to keep 3 instances running. Therefore, the node that I destroyed will fail the Autoscale Group Healthcheck and a new instance will be spawned to replace it.</p>

<p>Using the ElasticSearch <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html">Cluster Health API</a>, I can determine when the new node is in place:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?wait_for_nodes=3&amp;timeout=100s&#39;
</code></pre></div>
<p>The command will continue running until the cluster has 3 nodes in it. If you want to replace more nodes in the cluster, then repeat the steps above. If you are finished, then it is important to re-enable the shard reallocation:</p>
<div class="highlight"><pre><code class="text">curl -XPUT localhost:9200/_cluster/settings -d &#39;{
                &quot;transient&quot; : {
                    &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot;
                }
        }&#39;
</code></pre></div>
<p>The time taken to rebalance the cluster will depend on the number and size of the shards.</p>

<p>You can monitor the health of the cluster until it turns green:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?wait_for_status=green&amp;timeout=100s&#39;
</code></pre></div>
<p>The cluster is now green and all is working as expected again:</p>
<div class="highlight"><pre><code class="text">curl -XGET &#39;http://localhost:9200/_cluster/health?pretty=true&#39;
{
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 3,
  &quot;number_of_data_nodes&quot; : 3,
  &quot;active_primary_shards&quot; : 160,
  &quot;active_shards&quot; : 470,
  &quot;relocating_shards&quot; : 1,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0
}
</code></pre></div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Kibana Using Nginx as an SSL Proxy]]></title>
    <link href="http://paulstack.co.uk/blog/2016/01/03/deploying-kibana-using-nginx-as-an-ssl-proxy/"/>
    <updated>2016-01-03T01:16:00+00:00</updated>
    <id>http://paulstack.co.uk/blog/2016/01/03/deploying-kibana-using-nginx-as-an-ssl-proxy</id>
    <content type="html"><![CDATA[<p>In my last <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">post</a>, I described how I use <a href="https://packer.io/">Packer</a> and <a href="https://terraform.io/">Terraform</a> to deploy an <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> cluster. In order to make the logs stored in ElasticSearch searchable, I use <a href="https://www.elastic.co/products/kibana">Kibana</a>. I follow the previous pattern and deploy Kibana using Packer to build an <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.htm">AMI</a> and then create the infrastructure using Terraform. The Packer template has already taken into account that I want to use <a href="https://www.nginx.com/">nginx</a> as a proxy. </p>

<h3>Building Kibana AMIs with Packer and Ansible</h3>

<p>The template looks as follows:</p>

<p>
<code>json 
{
  &quot;variables&quot;: {
    &quot;ami_id&quot;: &quot;&quot;,
    &quot;private_subnet_id&quot;: &quot;&quot;,
    &quot;security_group_id&quot;: &quot;&quot;,
    &quot;packer_build_number&quot;: &quot;&quot;,
  },
  &quot;description&quot;: &quot;Kibana Image&quot;,
  &quot;builders&quot;: [
    {
      &quot;ami_name&quot;: &quot;kibana-{{user `packer_build_number`}}&quot;,
      &quot;availability_zone&quot;: &quot;eu-west-1a&quot;,
      &quot;iam_instance_profile&quot;: &quot;app-server&quot;,
      &quot;instance_type&quot;: &quot;t2.small&quot;,
      &quot;region&quot;: &quot;eu-west-1&quot;,
      &quot;run_tags&quot;: {
        &quot;role&quot;: &quot;packer&quot;
      },
      &quot;security_group_ids&quot;: [
        &quot;{{user `security_group_id`}}&quot;
      ],
      &quot;source_ami&quot;: &quot;{{user `ami_id`}}&quot;,
      &quot;ssh_timeout&quot;: &quot;10m&quot;,
      &quot;ssh_username&quot;: &quot;ubuntu&quot;,
      &quot;subnet_id&quot;: &quot;{{user `private_subnet_id`}}&quot;,
      &quot;tags&quot;: {
        &quot;Name&quot;: &quot;kibana-packer-image&quot;
      },
      &quot;type&quot;: &quot;amazon-ebs&quot;
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;inline&quot;: [ &quot;sleep 10&quot; ]
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;install_dependencies.sh&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;&#39; | {{ .Vars }} sudo -E -S sh &#39;{{ .Path }}&#39;&quot;
    },
    {
      &quot;type&quot;: &quot;ansible-local&quot;,
      &quot;playbook_file&quot;: &quot;kibana.yml&quot;,
      &quot;extra_arguments&quot;: [
        &quot;--module-path=./modules&quot;
      ],
      &quot;playbook_dir&quot;: &quot;../../&quot;
    }
  ]
}
</code>
 </p>

<p>The install_dependencies.sh script is as described <a href="http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/">previously</a></p>

<p>The ansible playbook for Kibana looks as follows:  </p>

<p>
```yaml
- hosts: all
  sudo: yes</p>

<p>pre<em>tasks:
    - ec2</em>tags:
    - ec2_facts:</p>

<p>roles:
    - base
    - kibana
    - reverse_proxied
```
</p>

<p>The playbook installs a base role for all the base pieces of my system (e.g. Logstash, Sensu-client, prometheus node_exporter) and then proceeds to install ElasticSearch.</p>

<p>The Kibana role looks as follows:</p>

<p><br>
```yaml<br>
- name: Download Kibana
  get<em>url: url=https://download.elasticsearch.org/kibana/kibana/kibana-{{ kibana</em>version }}-linux-x64.tar.gz dest=/tmp/kibana-{{ kibana_version }}-linux-x64.tar.gz mode=0440</p>

<ul>
<li><p>name: Untar Kibana
command: tar xzf /tmp/kibana-{{ kibana<em>version }}-linux-x64.tar.gz -C /opt creates=/opt/kibana-{{ kibana</em>version }}-linux-x64.tar.gz</p></li>
<li><p>name: Link to Kibana Directory
file: src=/opt/kibana-{{ kibana_version }}-linux-x64
    dest=/opt/kibana
    state=link
    force=yes</p></li>
<li><p>name: Link Kibana to ElasticSearch
lineinfile: &gt;
dest=/opt/kibana/config/kibana.yml
regexp=&quot;^elasticsearch<em>url:&quot;
line=&#39;elasticsearch</em>url: &quot;{{ elasticsearch_url }}&quot;&#39;</p></li>
<li><p>name: Create Kibana Init Script
copy: src=initd.conf dest=/etc/init.d/kibana mode=755 owner=root</p></li>
<li><p>name: Ensure Kibana is running
service: name=kibana state=started
```
</p></li>
</ul>

<p>The reverse_proxied ansible role looks as follows:</p>

<p>
```yaml
- name: download private key file
  command: aws s3 cp {{ reverse<em>proxy</em>private<em>key</em>s3<em>path }} /etc/ssl/private/{{ reverse</em>proxy<em>private</em>key }}</p>

<ul>
<li><p>name: private key permissions
file: path=/etc/ssl/private/{{ reverse<em>proxy</em>private_key }} mode=600</p></li>
<li><p>name: download certificate file
command: aws s3 cp {{ reverse<em>proxy</em>cert<em>s3</em>path }} /etc/ssl/certs/{{ reverse<em>proxy</em>cert }}</p></li>
<li><p>name: download DH 2048bit encryption
command: aws s3 cp {{ reverse<em>proxy</em>dh<em>pem</em>s3<em>path }} /etc/ssl/{{ reverse</em>proxy<em>dh</em>pem }}</p></li>
<li><p>name: certificate permissions
file: path=/etc/ssl/certs/{{ reverse<em>proxy</em>cert }} mode=644</p></li>
<li><p>apt: pkg=nginx</p></li>
<li><p>name: remove default nginx site from sites-emabled
file: path=/etc/nginx/sites-enabled/default state=absent</p></li>
<li><p>template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf mode=644 owner=root group=root</p></li>
<li><p>service: name=nginx state=restarted</p></li>
<li><p>file: path=/var/log/nginx
    mode=0755
    state=directory
```
</p></li>
</ul>

<p>This role downloads a private SSL Key and a Certificate from a <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html">S3 bucket</a> that is security controlled through <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">IAM</a>. This allows us to configure nginx to act as a proxy. The nginx proxy template is available to <a href="https://gist.github.com/stack72/1d76839c6783bd7eea33">view</a>.</p>

<p>We can then pass a number of variables to our role for use within ansible:</p>

<p>
<code>yaml
reverse_proxy_private_key: mydomain.key
reverse_proxy_private_key_s3_path: s3://my-bucket/certs/mydomain/mydomain.key
reverse_proxy_cert: mydomain.crt
reverse_proxy_cert_s3_path: s3://my-bucket/certs/mydomain/mydomain.crt
reverse_proxy_dh_pem_s3_path: s3://my-bucket/certs/dhparams.pem
reverse_proxy_dh_pem: dhparams.pem
proxy_urls:
  - reverse_proxy_url: /
    reverse_proxy_upstream_port: 3000
kibana_version: 4.1.0
elasticsearch_url: http://myes.com:9200
</code>
</p>

<p>This allows me to easily change the configuration of nginx to patch security vulnerabilities easily.</p>

<h3>Deploying Kibana with Terraform</h3>

<p>The infrastructure of the Kibana cluster is now pretty easy. The Terraform script now looks as follows:</p>
<div class="highlight"><pre><code class="text">resource &quot;aws_security_group&quot; &quot;kibana&quot; {
  name = &quot;kibana-sg&quot;
  description = &quot;Kibana Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 443
    to_port   = 443
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  }

  ingress {
    from_port = 80
    to_port   = 80
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;Kibana Node&quot;
  }
}

resource &quot;aws_security_group&quot; &quot;kibana_elb&quot; {
  name = &quot;kibana-elb-sg&quot;
  description = &quot;Kibana Elastic Load Balancer Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 443
    to_port   = 443
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  ingress {
    from_port = 80
    to_port   = 80
    protocol  = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;Kibana Load Balancer&quot;
  }
}

resource &quot;aws_elb&quot; &quot;kibana_elb&quot; {
  name = &quot;kibana-elb&quot;
  subnets = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  security_groups = [&quot;${aws_security_group.kibana_elb.id}&quot;]
  cross_zone_load_balancing = true
  connection_draining = true
  internal = true

  listener {
    instance_port      = 443
    instance_protocol  = &quot;tcp&quot;
    lb_port            = 443
    lb_protocol        = &quot;tcp&quot;
  }

  listener {
    instance_port      = 80
    instance_protocol  = &quot;tcp&quot;
    lb_port            = 80
    lb_protocol        = &quot;tcp&quot;
  }

  health_check {
    healthy_threshold   = 2
    unhealthy_threshold = 2
    interval            = 10
    target              = &quot;TCP:443&quot;
    timeout             = 5
  }
}

resource &quot;aws_launch_configuration&quot; &quot;kibana_launch_config&quot; {
  image_id = &quot;${var.kibana_ami_id}&quot;
  instance_type = &quot;${var.kibana_instance_type}&quot;
  iam_instance_profile = &quot;app-server&quot;
  key_name = &quot;${aws_key_pair.terraform.key_name}&quot;
  security_groups = [&quot;${aws_security_group.kibana.id}&quot;,&quot;${aws_security_group.node.id}&quot;]
  enable_monitoring = false

  root_block_device {
    volume_size = &quot;${var.kibana_volume_size}&quot;
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource &quot;aws_autoscaling_group&quot; &quot;kibana_autoscale_group&quot; {
  name = &quot;kibana-autoscale-group&quot;
  availability_zones = [&quot;${aws_subnet.primary-private.availability_zone}&quot;,&quot;${aws_subnet.secondary-private.availability_zone}&quot;,&quot;${aws_subnet.tertiary-private.availability_zone}&quot;]
  vpc_zone_identifier = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  launch_configuration = &quot;${aws_launch_configuration.kibana_launch_config.id}&quot;
  min_size = 2
  max_size = 100
  health_check_type = &quot;EC2&quot;
  load_balancers = [&quot;${aws_elb.kibana_elb.name}&quot;]

  tag {
    key = &quot;Name&quot;
    value = &quot;kibana&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;role&quot;
    value = &quot;kibana&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_name&quot;
    value = &quot;${aws_elb.kibana_elb.name}&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_region&quot;
    value = &quot;${var.aws_region}&quot;
    propagate_at_launch = true
  }
}
</code></pre></div>
<p>This allows me to scale my system up or down just by changing the values in my Terraform configuration. When the instances are instantiated, the Kibana instances are added to the ELB and they are then available to serve traffic</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building an ElasticSearch cluster in AWS with Packer and Terraform]]></title>
    <link href="http://paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform/"/>
    <updated>2016-01-02T12:47:00+00:00</updated>
    <id>http://paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform</id>
    <content type="html"><![CDATA[<p>As discussed in a <a href="http://www.paulstack.co.uk/blog/2015/11/09/the-quest-for-infra-management-2-dot-0/">previous post</a>, I like to build separate <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">AMIs</a> for each of my systems. This allows me to scale up and recycle nodes easily. I have been doing this with <a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a> for a while now. I usually build an AMI with <a href="https://packer.io/">Packer</a> and <a href="http://www.ansible.com/">Ansible</a> and I use <a href="https://terraform.io/">Terraform</a> to roll out the infrastructure</p>

<h3>Building ElasticSearch AMIs with Packer</h3>

<p>The packer template looks as follows:</p>

<p>
<code>json 
{
  &quot;variables&quot;: {
    &quot;ami_id&quot;: &quot;&quot;,
    &quot;private_subnet_id&quot;: &quot;&quot;,
    &quot;security_group_id&quot;: &quot;&quot;,
    &quot;packer_build_number&quot;: &quot;&quot;,
  },
  &quot;description&quot;: &quot;ElasticSearch Image&quot;,
  &quot;builders&quot;: [
    {
      &quot;ami_name&quot;: &quot;elasticsearch-{{user `packer_build_number`}}&quot;,
      &quot;availability_zone&quot;: &quot;eu-west-1a&quot;,
      &quot;iam_instance_profile&quot;: &quot;app-server&quot;,
      &quot;instance_type&quot;: &quot;t2.small&quot;,
      &quot;region&quot;: &quot;eu-west-1&quot;,
      &quot;run_tags&quot;: {
        &quot;role&quot;: &quot;packer&quot;
      },
      &quot;security_group_ids&quot;: [
        &quot;{{user `security_group_id`}}&quot;
      ],
      &quot;source_ami&quot;: &quot;{{user `ami_id`}}&quot;,
      &quot;ssh_timeout&quot;: &quot;10m&quot;,
      &quot;ssh_username&quot;: &quot;ubuntu&quot;,
      &quot;subnet_id&quot;: &quot;{{user `private_subnet_id`}}&quot;,
      &quot;tags&quot;: {
        &quot;Name&quot;: &quot;elasticsearch-packer-image&quot;
      },
      &quot;type&quot;: &quot;amazon-ebs&quot;
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;inline&quot;: [ &quot;sleep 10&quot; ]
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;install_dependencies.sh&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;&#39; | {{ .Vars }} sudo -E -S sh &#39;{{ .Path }}&#39;&quot;
    },
    {
      &quot;type&quot;: &quot;ansible-local&quot;,
      &quot;playbook_file&quot;: &quot;elasticsearch.yml&quot;,
      &quot;extra_arguments&quot;: [
        &quot;--module-path=./modules&quot;
      ],
      &quot;playbook_dir&quot;: &quot;../../&quot;
    }
  ]
}
</code>
</p>

<p>This is essentially a pretty simple script and builds an AWS Instance in a private subnet of my choice in eu-west-1a in AWS. </p>

<h4>install_dependencies.sh</h4>

<p>The first part of the script just installs the dependencies that my system has:</p>

<p>
```bash</p>

<h1>!/bin/bash</h1>

<p>apt-get update
apt-get upgrade -y
apt-get install -y software-properties-common git
apt-add-repository -y ppa:ansible/ansible apt-get update</p>

<h1>workaround for ubuntu pip bug - https://bugs.launchpad.net/ubuntu/+source/python-pip/+bug/1306991</h1>

<p>rm -rf /usr/local/lib/python2.7/dist-packages/requests
apt-get install -y python-dev</p>

<p>ssh-keyscan -H github.com &gt; /etc/ssh/ssh<em>known</em>hosts</p>

<p>wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py
python get-pip.py</p>

<p>pip install ansible paramiko PyYAML jinja2 httplib2 netifaces boto awscli six
```
</p>

<h4>Ansible playbook for ElasticSearch</h4>

<p>The ElasticSearch playbook looks as follows:</p>

<p>
```YAML<br>
- hosts: all
  sudo: yes</p>

<p>pre<em>tasks:
    - ec2</em>tags:
    - ec2_facts:</p>

<p>roles:
    - base
    - elasticsearch
```
</p>

<p>The playbook installs a base role for all the base pieces of my system (e.g. Logstash, Sensu-client, prometheus node_exporter) and then proceeds to install ElasticSearch. </p>

<p>The ElasticSearch role looks as follows:</p>

<p>
```YAML
- ec2<em>facts:
- ec2</em>tags:</p>

<ul>
<li><p>name: Add Oracle Java Repository
apt_repository: repo=&#39;ppa:webupd8team/java&#39;</p></li>
<li><p>name: Accept Java 8 Licence
shell: echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | tee /etc/oracle-java-8-licence-acceptance | /usr/bin/debconf-set-selections
args:
creates: /etc/oracle-java-8-licence-acceptance</p></li>
<li><p>name: Add ElasticSearch repo public signing key
apt_key: id=46095ACC8548582C1A2699A9D27D666CD88E42B4 url=https://packages.elastic.co/GPG-KEY-elasticsearch state=present</p></li>
<li><p>name: Add ElasticSearch repository
apt<em>repository:
repo: &#39;deb http://packages.elasticsearch.org/elasticsearch/{{ es</em>release }}/debian stable main&#39;
state: present</p></li>
<li><p>name: Install Oracle Java 8
apt: name={{item}} state=latest
with_items:</p>

<ul>
<li>oracle-java8-installer</li>
<li>ca-certificates</li>
<li>oracle-java8-set-default</li>
</ul></li>
<li><p>name: Install ElasticSearch
apt: name=elasticsearch={{ es_version }} state=present
notify: Restart elasticsearch</p></li>
<li><p>name: Copy /etc/default/elasticsearch
template: src=elasticsearch dest=/etc/default/elasticsearch
notify: Restart elasticsearch</p></li>
<li><p>name: Copy /etc/elasticsearch/elasticsearch.yml
template: src=elasticsearch.yml dest=/etc/elasticsearch/elasticsearch.yml
notify: Restart elasticsearch</p></li>
<li><p>name: Set elasticsearch service to start on boot
service: name=elasticsearch enabled=yes</p></li>
<li><p>name: Install plugins
command: bin/plugin --install {{item.name}}
args:
chdir: &quot;{{ es<em>home }}&quot;
creates: &quot;{{ es</em>home }}/plugins/{{ item.plugin<em>file | default(item.name.split(&#39;/&#39;)[1]) }}&quot;
with</em>items: es_plugins
notify: Restart elasticsearch</p></li>
<li><p>name: Set elasticsearch to be running
service: name=elasticsearch state=running enabled=yes</p></li>
</ul>
<div class="highlight"><pre><code class="text">

This is just some basic ansible commands to get the apt-repo, packages and plugins installed in the system. You can find the templates used [here](https://gist.github.com/stack72/bdef4126ae8b08214bd8). The important part to note is that variables are used both in the script **and** in the templates to setup the cluster to the required level.

My variables look as follows:
</code></pre></div>
<p>es<em>release: &quot;1.6&quot;
es</em>version: &quot;.0&quot;
es</em>home: /usr/share/elasticsearch
es<em>wait</em>for<em>listen: yes
es</em>etc:
  cluster<em>name: central</em>logging<em>cluster
  discovery.type: ec2
  discovery.ec2.groups: elasticsearch-sg
  cloud.aws.region: &quot;&quot;
es<em>default</em>es<em>heap</em>size: 4g
es<em>plugins:
  - name: elasticsearch/elasticsearch-cloud-aws/2.6.0
  - name: elasticsearch/marvel/latest
  - name: mobz/elasticsearch-head
es</em>etc<em>index</em>number<em>of</em>replicas: 2
```</p>

<p>As I have specified <code>elasticsearch-sg</code> and installed the <code>elasticsearch-cloud-aws</code> plugin, my nodes can auto-discover each other in the aws region. I can build the packer image as follows:</p>
<div class="highlight"><pre><code class="text">#!/bin/bash

LATEST_UBUNTU_IMAGE=$(curl http://cloud-images.ubuntu.com/locator/ec2/releasesTable | grep eu-west-1 | grep trusty | grep amd64 | grep &quot;\&quot;hvm:ebs\&quot;&quot; | awk -F &quot;[&lt;&gt;]&quot; &#39;{print $3}&#39;)

packer build \
  -var ami_id=$LATEST_UBUNTU_IMAGE \
  -var security_group_id=MYSGID\
  -var private_subnet_id=MYSUBNETID \
  -var packer_build_number=PACKERBUILDNUMBER \
  elasticsearch.json
</code></pre></div>
<p>We are now ready to build the infrastructure for the cluster</p>

<h3>Building an ElasticSearch Cluster with Terraform</h3>

<p>The infrastructure of the ElasticSearch cluster is now pretty easy. I deploy my nodes into a <a href="https://aws.amazon.com/vpc/">VPC</a> and onto private subnets so that they are not externally accessible. I have an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB</a> in place across the nodes so that I can easily get to the ElasticSearch plugins like <a href="https://www.elastic.co/guide/en/marvel/current/index.html">Marvel</a> and <a href="https://mobz.github.io/elasticsearch-head/">Head</a>.</p>

<p>The Terraform configuration is as follows:  </p>
<div class="highlight"><pre><code class="text">resource &quot;aws_security_group&quot; &quot;elasticsearch&quot; {
  name = &quot;elasticsearch-sg&quot;
  description = &quot;ElasticSearch Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 9200
    to_port   = 9400
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.elasticsearch_elb.id}&quot;]
  }

  ingress {
    from_port = 9200
    to_port   = 9400
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.node.id}&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;ElasticSearch Node&quot;
  }
}


resource &quot;aws_security_group&quot; &quot;elasticsearch_elb&quot; {
  name = &quot;elasticsearch-elb-sg&quot;
  description = &quot;ElasticSearch Elastic Load Balancer Security Group&quot;
  vpc_id = &quot;${aws_vpc.default.id}&quot;

  ingress {
    from_port = 9200
    to_port   = 9200
    protocol  = &quot;tcp&quot;
    security_groups = [&quot;${aws_security_group.node.id}&quot;]
  }

  egress {
    from_port = &quot;0&quot;
    to_port = &quot;0&quot;
    protocol = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;ElasticSearch Load Balancer&quot;
  }
}

resource &quot;aws_elb&quot; &quot;elasticsearch_elb&quot; {
  name = &quot;elasticsearch-elb&quot;
  subnets = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  security_groups = [&quot;${aws_security_group.elasticsearch_elb.id}&quot;]
  cross_zone_load_balancing = true
  connection_draining = true
  internal = true

  listener {
    instance_port      = 9200
    instance_protocol  = &quot;tcp&quot;
    lb_port            = 9200
    lb_protocol        = &quot;tcp&quot;
  }

  health_check {
    healthy_threshold   = 2
    unhealthy_threshold = 2
    interval            = 10
    target              = &quot;TCP:9200&quot;
    timeout             = 5
  }
}

resource &quot;aws_autoscaling_group&quot; &quot;elasticsearch_autoscale_group&quot; {
  name = &quot;elasticsearch-autoscale-group&quot;
  availability_zones = [&quot;${aws_subnet.primary-private.availability_zone}&quot;,&quot;${aws_subnet.secondary-private.availability_zone}&quot;,&quot;${aws_subnet.tertiary-private.availability_zone}&quot;]
  vpc_zone_identifier = [&quot;${aws_subnet.primary-private.id}&quot;,&quot;${aws_subnet.secondary-private.id}&quot;,&quot;${aws_subnet.tertiary-private.id}&quot;]
  launch_configuration = &quot;${aws_launch_configuration.elasticsearch_launch_config.id}&quot;
  min_size = 3
  max_size = 100
  desired = 3
  health_check_grace_period = &quot;900&quot;
  health_check_type = &quot;EC2&quot;
  load_balancers = [&quot;${aws_elb.elasticsearch_elb.name}&quot;]

  tag {
    key = &quot;Name&quot;
    value = &quot;elasticsearch&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;role&quot;
    value = &quot;elasticsearch&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_name&quot;
    value = &quot;${aws_elb.elasticsearch_elb.name}&quot;
    propagate_at_launch = true
  }

  tag {
    key = &quot;elb_region&quot;
    value = &quot;${var.aws_region}&quot;
    propagate_at_launch = true
  }
}

resource &quot;aws_launch_configuration&quot; &quot;elasticsearch_launch_config&quot; {
  image_id = &quot;${var.elasticsearch_ami_id}&quot;
  instance_type = &quot;${var.elasticsearch_instance_type}&quot;
  iam_instance_profile = &quot;app-server&quot;
  key_name = &quot;${aws_key_pair.terraform.key_name}&quot;
  security_groups = [&quot;${aws_security_group.elasticsearch.id}&quot;,&quot;${aws_security_group.node.id}&quot;]
  enable_monitoring = false

  lifecycle {
    create_before_destroy = true
  }

  root_block_device {
    volume_size = &quot;${var.elasticsearch_volume_size}&quot;
  }
}
</code></pre></div>
<p>This allows me to scale my system up or down just by changing the values in my Terraform configuration. When the instances are instantiatied, the ElasticSearch cloud plugin discovers the other members of the cluster and allows the node to join the cluster</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Autoscaling Group Notifications with Terraform and AWS Lambda]]></title>
    <link href="http://paulstack.co.uk/blog/2015/12/30/autoscaling-group-notifications-with-terraform-and-aws-lambda/"/>
    <updated>2015-12-30T13:51:00+00:00</updated>
    <id>http://paulstack.co.uk/blog/2015/12/30/autoscaling-group-notifications-with-terraform-and-aws-lambda</id>
    <content type="html"><![CDATA[<p>I use Autoscaling Groups in AWS for all of my systems. The main benefit for me here was to make sure that when a node died in AWS, the Autoscaling Group policy made sure that the node was replaced. I wanted to get some visibility of when the Autoscaling Group was launching and terminating nodes and decided that posting notifications to <a href="https://slack.com/">Slack</a> would be a good way of getting this. With <a href="https://terraform.io/">Terraform</a> and <a href="http://docs.aws.amazon.com/lambda/latest/dg/welcome.html">AWS Lambda</a>, I was able to make this happen.</p>

<p><strong>This post assumes that you are already setup and running with Terraform</strong></p>

<p>Create an IAM Role that allows access to AWS Lambda:</p>
<div class="highlight"><pre><code class="text">resource &quot;aws_iam_role&quot; &quot;slack_iam_lambda&quot; {
    name = &quot;slack-iam-lambda&quot;
    assume_role_policy = &lt;&lt;EOF
{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
      &quot;Principal&quot;: {
        &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;
      },
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Sid&quot;: &quot;&quot;
    }
  ]
}
EOF
}
</code></pre></div>
<p>Create a <a href="GIST%20GOES%20HERE">lambda function</a> as follows:</p>
<div class="highlight"><pre><code class="text">resource &quot;aws_lambda_function&quot; &quot;slack_notify&quot; {
  filename = &quot;slackNotify.zip&quot;
  function_name = &quot;slackNotify&quot;
  role = &quot;${aws_iam_role.slack_iam_lambda.arn}&quot;
  handler = &quot;slackNotify.handler&quot;
}
</code></pre></div>
<p>We assume here, that you have already created a Slack Integration. The hook URL from that integration is required for the lambda contents. </p>

<p>The filename <code>slackNotify.zip</code> is a zip of a file called <code>slackNotify.js</code>. The contents of that js file are <a href="https://gist.github.com/stack72/ad97da2df376754e413a">available</a></p>

<p>Terraform currently does not support hooking AWS Lambda up to SNS Event Sources. Therefore, unfortunately, there is a manual step required to configure the Lambda to talk to the SNS Topic. There is a PR in Terraform to allow this to be automated as well.</p>

<p>In the AWS Console, go to Lambda and then chose the Lambda function. </p>

<p><img src="/images/lambda_function.png" alt="Image"></p>

<p>Go to the <code>Event Sources</code> tab:</p>

<p><img src="/images/lambda_function_event_sources.png" alt="Image"></p>

<p>Click on <code>Add Event Source</code> and then choose <code>SNS</code> from the dropdown and then make sure you chose the correct SNS Topic name:</p>

<p><img src="/images/lambda_function_sns_topic.png" alt="Image"></p>

<p>We then use another Terraform resource to attach the Autoscale Groups to the Lambda as follows:</p>
<div class="highlight"><pre><code class="text">resource &quot;aws_autoscaling_notification&quot; &quot;slack_notifications&quot; {
  group_names = [
    &quot;admin-api-autoscale-group&quot;,
    &quot;rundeck-autoscale-group&quot;,
  ]
  notifications  = [
    &quot;autoscaling:EC2_INSTANCE_LAUNCH&quot;,
    &quot;autoscaling:EC2_INSTANCE_TERMINATE&quot;,
    &quot;autoscaling:EC2_INSTANCE_LAUNCH_ERROR&quot;,
    &quot;autoscaling:EC2_INSTANCE_TERMINATE_ERROR&quot;,
    &quot;autoscaling:TEST_NOTIFICATION&quot;
  ]
  topic_arn = &quot;${aws_sns_topic.asg_slack_notifications.arn}&quot;
}
</code></pre></div>
<p>As we have configured notifications for autoscaling:TEST_NOTIFICATION, when you apply this configuration with Terraform, you will see something similar to the following in Slack:</p>

<p><img src="/images/slack_test_notification.png" alt="Image"></p>

<p>In the current infrastructure I manage, there are 27 Autoscale groups. I don&#39;t really want to add 27 hardcoded group<em>names in the aws</em>autoscaling_notifcation in Terraform. </p>

<p>I wanted to take advantage of a <a href="https://www.terraform.io/docs/modules/usage.html">Terraform module</a>. In a nutshell, the module does a lookup of all the Autoscaling Groups in a region and then passes that list into the Terraform resource.</p>

<p>The output of the <a href="https://github.com/stack72/tf_aws_autoscalegroup_names">module</a> looks as follows:</p>
<div class="highlight"><pre><code class="text">{
  &quot;variable&quot;: {
    &quot;autoscalegroup_names&quot;: {
      &quot;description&quot;: &quot;List of autoscalegroup names for a region&quot;,
      &quot;default&quot;: {
        &quot;eu-west-1&quot;: &quot;admin-api-autoscale-group,dash-autoscale-group,demo-autoscale-group,docker-v2-autoscale-group,elasticsearch-autoscale-group,faces-autoscale-group,internal-api-autoscale-group,jenkins-master-autoscale-group,kafka-autoscale-group,landscapes-autoscale-group&quot;,
        &quot;ap-southeast-1&quot;: &quot;&quot;,
        &quot;ap-southeast-2&quot;: &quot;&quot;,
        &quot;eu-central-1&quot;: &quot;&quot;,
        &quot;ap-northeast-1&quot;: &quot;&quot;,
        &quot;us-east-1&quot;: &quot;&quot;,
        &quot;sa-east-1&quot;: &quot;&quot;,
        &quot;us-west-1&quot;: &quot;&quot;,
        &quot;us-west-2&quot;: &quot;&quot;
      }
    }
  }
}
</code></pre></div>
<p>I then pass this into the Terraform resource as follows:</p>
<div class="highlight"><pre><code class="text">module &quot;autoscalegroups&quot; {
  source = &quot;github.com/stack72/tf_aws_autoscalegroup_names&quot;
  region = &quot;${var.aws_region}&quot;
}

resource &quot;aws_autoscaling_notification&quot; &quot;slack_notifications&quot; {
  group_names = [
    &quot;${split(&quot;,&quot;, module.autoscalegroups.asg_names)}&quot;,
  ]
  notifications  = [
    &quot;autoscaling:EC2_INSTANCE_LAUNCH&quot;,
    &quot;autoscaling:EC2_INSTANCE_TERMINATE&quot;,
    &quot;autoscaling:EC2_INSTANCE_LAUNCH_ERROR&quot;,
    &quot;autoscaling:EC2_INSTANCE_TERMINATE_ERROR&quot;,
    &quot;autoscaling:TEST_NOTIFICATION&quot;
  ]
  topic_arn = &quot;${aws_sns_topic.asg_slack_notifications.arn}&quot;
}
</code></pre></div>
<p>When anything happens within an Autoscaling Group, I now get notifications as follows:</p>

<p><img src="/images/termination_notification.png" alt="Image">
<img src="/images/launch_notification.png" alt="Image"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Quest for Infrastructure Management 2.0]]></title>
    <link href="http://paulstack.co.uk/blog/2015/11/09/the-quest-for-infra-management-2-dot-0/"/>
    <updated>2015-11-09T10:50:00+00:00</updated>
    <id>http://paulstack.co.uk/blog/2015/11/09/the-quest-for-infra-management-2-dot-0</id>
    <content type="html"><![CDATA[<p>I&#39;ve long been a configuration management tool fan. I have blogged, spoken at conferences and used <a href="https://puppetlabs.com/">Puppet</a> as well as <a href="https://www.chef.io/">Chef</a> and <a href="http://www.ansible.com/">Ansible</a>. The more I use these tools now, the more I realise I&#39;m actually not making my life any easier</p>

<p>Currently, the infrastructure I manage is 100% AWS Cloud based. This has actually changed how I work:</p>

<ol>
<li><p>I have learned to always expect problems so I therefore should have <em>everything</em> 100% automated.</p></li>
<li><p>No server is kept in production for more than 2 weeks</p></li>
</ol>

<p>By combining these 2 ways of working, I can easily recover from outages. The speed of recovery is down to being able to provision the pieces of my system as fast as possible. The simplist way to be able to provision instances fast is to build my own AMIs with <a href="https://packer.io/">Packer</a>. I have come to the realisation that when I boot an instance, I don&#39;t <em>really</em> want to wait for a configuration management tool to run. I have also begun to realise that having a tool change my systems in production can introduce unneeded risk. The Packer templates to build the AMI have <a href="http://serverspec.org/">serverspec</a> tests built into them. This means that at build time, I know if an AMI has been built correctly.</p>

<p>The AWS infrastructure itself is managed by <a href="https://terraform.io/">Terraform</a>. I tend to use AutoScalingGroups and LaunchConfig for the instances and when Terraform is checking the state of the infrastructure, it will look up the latest AMI ID and make sure that it is part of the Launch Configuration. If it isn&#39;t, Terraform will update the Launch Config so that the next machine will be booted from the new AMI.</p>

<p>I use <a href="http://rundeck.org/">Rundeck</a> for orchestrating changes to the infrastructure. I have a job in Rundeck that allows me to recycle <em>all</em> instances in an AutoScalingGroup one at a time and in a HA manner. From building a new AMI, to fully recycling an AutoScalingGroup is about 20 minutes (the packer build itself takes about 12 minutes). So, in theory, it takes me about 20 minutes to release new security patches to all instances in an AutoScalingGroup.</p>

<h3>Isn&#39;t this just &#39;Golden Images&#39;?</h3>

<p>Technically, yes. But the important for me is being able to roll out a fully tested AMI and then not making any additional changes to it in production. I would like to say that my infrastructure is 100% immutable, but after reading a <a href="https://medium.com/@elijahz/what-version-is-your-infrastructure-3a61fe804d0e">recent article</a> by <a href="https://twitter.com/emmajanehw">@emmajanehw</a>, I now realise that can never be the case. Each of my AMIs are versioned and I have a nightly Rundeck job that tells me what version of an AMI a system is built / released with.</p>

<h3>Do I Consider Configuration Management Dead?</h3>

<p>Not at all. I simply do not want to make additional changes to my environments when I know they are working. Right now, I use Ansible to provision my AMI as part of my Packer scripts. So I do believe these tools still need to be part of our eco-system. I could substitute in any configuration management tool to help build my AMIs. The purists could even use bash / shell scripts to do the same job</p>

<h3>Can I only do this if I use *nix / AWS?</h3>

<p>Not at all. At $JOB[-1], we actually were changing our provisioning to allow us to spin up images much faster. We were using a mix of AMIs and VMWare templates for Windows and Ubuntu. By moving in that direction, it would reduce the time taken to provision a box from maybe an hour to minutes.</p>

<p>In my opinion, moving to a more immutable style of infrastructure is the next phase of infra management for me. I believe the learnings from using config management tools in production across 1000s of nodes has helped me move in this direction but YMMV.</p>
]]></content>
  </entry>
  
</feed>
